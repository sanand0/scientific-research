{
  "metadata": {
    "title": "GPT-4 Technical Report",
    "authors": [
      {
        "name": "OpenAI",
        "affiliations": [],
        "email": "gpt4-report@openai.com"
      }
    ],
    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4â€™s performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
    "keywords": null,
    "doi": "arXiv:2303.08774v6",
    "publication_info": {
      "journal": null,
      "year": "2024",
      "volume": null,
      "issue": null,
      "pages": null
    },
    "title_location": {
      "page": 1,
      "bbox": [
        216.72999572753906,
        99.8338394165039,
        395.27105712890625,
        117.04924011230469
      ],
      "found": true
    },
    "abstract_location": {
      "page": 1,
      "bbox": [
        143.39700317382812,
        225.5125732421875,
        469.7836608886719,
        355.49407958984375
      ],
      "found": true
    }
  },
  "references": [],
  "extraction_method": "Hybrid (PyMuPDF + GPT-4)",
  "total_pages": 100
}